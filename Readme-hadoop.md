[]: # Title: Hadoop Movie Review Sentiment Analysis Using MapReduce
[]: # Description: This repository is designed to test MapReduce jobs using a dataset of movie reviews for sentiment analysis. Students are expected to classify reviews as either positive or negative based on predefined word lists.
[]: # Last Edited: 2021-09-29 12:00:00
# Assignment-2: Hadoop Movie Review Sentiment Analysis Using MapReduce

This repository is designed to test MapReduce jobs using a dataset of movie reviews for sentiment analysis. Students are expected to classify reviews as either positive or negative based on predefined word lists.

## Objectives

By completing this assignment, students will:

1. **Understand Sentiment Analysis:** Learn how to perform sentiment analysis by analyzing movie reviews using positive and negative word lists.
2. **Gain Experience with Hadoop's MapReduce:** Develop a MapReduce job in Java that processes large text datasets in a distributed environment.
3. **Deploy and Run a Hadoop Cluster with Docker:** Learn how to deploy a Hadoop cluster using Docker and run MapReduce jobs on it.
4. **Work with Docker Containers in Codespaces:** Understand how to use GitHub Codespaces and Docker to run and manage Hadoop components.
5. **Submit and Manage Code Using GitHub:** Develop skills in managing code and submitting assignments via GitHub.

---

## Setup and Execution

### 1. **Fork the GitHub Repository**

- First, accept the GitHub Classroom invitation and fork the assignment repository to your own GitHub account.
- Once you’ve forked the repo, open the repository in **GitHub Codespaces** to begin working on the assignment.

---

### 2. **Start the Hadoop Cluster Using Docker Compose**

The repository contains a `docker-compose.yml` file that configures a Hadoop cluster. Run the following command in GitHub Codespaces terminal to start the cluster:

```bash
docker-compose up -d
```

This will spin up the necessary Hadoop components (ResourceManager, NodeManager, etc.) inside Docker containers.

---

### 3. **Build the Java Code with Maven**

After starting the cluster, use Maven to build the Java MapReduce code for sentiment analysis. In the terminal, execute the following command to compile and build the project:

```bash
mvn install
```

This will generate a JAR file in the `target/` directory. This JAR file will contain your MapReduce code.

---

### 4. **Prepare Input Data Files**

1. The input movie reviews dataset is located in the `input/` folder of the repository. There are also two files, `positive_words.txt` and `negative_words.txt`, used for classifying the reviews.

2. Ensure all the input files (movie reviews, positive words, and negative words) are present in the `input/` directory.

---

### 5. **Move the JAR and Input Files to the Docker Container**

#### **5.1 Move the JAR File to the Container**

Copy the built JAR file to the ResourceManager container. Run this command:

**Note**: Make sure you replace `<your-jar-file>` with the actual name of the JAR file generated by Maven.
```bash
docker cp target/<your-jar-file>.jar resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

#### **5.2 Move the Input Files to the Container**

Next, copy the movie reviews dataset and the word list files to the ResourceManager container:

```bash
docker cp input/movie_reviews.csv resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
docker cp input/positive_words.txt resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
docker cp input/negative_words.txt resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### 6. **Connect to the ResourceManager Container**

To run the Hadoop commands, you'll need to connect to the ResourceManager container:

```bash
docker exec -it resourcemanager /bin/bash
```

Once inside the container, navigate to the Hadoop directory where your files were copied:

```bash
cd /opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### 7. **Set Up HDFS for Input Files**

To run the MapReduce job, the input files need to be stored in Hadoop’s distributed file system (HDFS).

#### **7.1 Create Directories in HDFS**

Create a directory in HDFS for the input files:

```bash
hadoop fs -mkdir -p /input/dataset
```

#### **7.2 Upload the Input Files to HDFS**

Upload the movie reviews and word list files to HDFS:

```bash
hadoop fs -put movie_reviews.txt /input/dataset/
hadoop fs -put positive_words.txt /input/dataset/
hadoop fs -put negative_words.txt /input/dataset/
```

---

### 8. **Execute the Sentiment Analysis MapReduce Job**

Now you are ready to run your MapReduce job for sentiment analysis.

Run the job using the following command:

**Note**: Make sure you replace `<your-jar-file>` with the actual name of the JAR file generated by Maven.

```bash
hadoop jar <your-jar-file>.jar com.sentimentanalysis.hadoop.SentimentAnalysis /input/dataset/movie_reviews.txt /output
```

This command will execute the MapReduce job with the movie reviews as the input and store the results in the `/output` directory in HDFS.

---

### 9. **View the Output of the MapReduce Job**

**Note**: The output will be stored in multiple files, so you may need to view the contents of each file.

To List the output files, use the following command:

```bash
hadoop fs -ls /output
```

To check the output of the job, you can use the following command to view the content of the output files:

**Note**: Replace `/output/part-r-00000` with the actual output file name that you will see in the output
of the previous command.

```bash
hadoop fs -cat /output/part-r-00000
```

This will display the number of positive and negative reviews identified by your sentiment analysis job.

---

### 10. **Copy Output from HDFS to Local OS**

Once you have verified the results, copy the output from HDFS to your local file system.

#### **10.1 Copy Output from HDFS**

Use the following command to copy the output from HDFS to the Hadoop directory:

```bash
hadoop fs -get /output /opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

#### **10.2 Copy Output from the Container to Your Local Machine**

Now, exit the ResourceManager container:

```bash
exit
```

Next, copy the output files from the Docker container to your GitHub Codespaces environment:

```bash
docker cp resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/output/ ./output/
```

---

### 11. **Submit Your Code and Output**

#### **11.1 Push Your Code and Output to GitHub**

Commit your changes, including the output from the MapReduce job, and push them to your GitHub repository:

```bash
git add .
git commit -m "Completed Sentiment Analysis Assignment"
git push origin main
```

#### **11.2 Submit the Assignment on GitHub Classroom**

Once you've pushed your code, go to GitHub Classroom and ensure your repository is submitted for the assignment. Make sure that the following are included:

1. The JAR file with your MapReduce job.
2. The input files (movie reviews, positive and negative words).
3. The output files from your MapReduce job.
4. The one-page report documenting the steps you followed, any challenges faced, and your observations.

---

### **Grading Criteria:**

1. **Correct Implementation of Sentiment Analysis:** The MapReduce job must correctly classify movie reviews based on the word lists provided.
2. **Proper Use of Hadoop and Docker:** Your solution must successfully deploy a Hadoop cluster using Docker and correctly manage input/output files using HDFS.
3. **Submission of Code and Output:** The correct output should be produced and submitted via GitHub, along with the code and a brief report.
4. **Report:** A clear one-page report summarizing your setup, challenges, and observations.

---

This concludes the instructions for the assignment. If you encounter any issues, feel free to reach out during office hours or post your queries in the course discussion forum.

Good luck, and happy coding!

---